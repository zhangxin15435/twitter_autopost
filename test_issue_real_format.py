#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•çœŸå®Issueæ ¼å¼çš„å†…å®¹è§£æ
é‡ç°ç”¨æˆ·æŠ¥å‘Šçš„å‘å¸ƒä¸å®Œæ•´é—®é¢˜
"""

import re
import json

def parse_issue_content_exact(content):
    """ä¸å·¥ä½œæµå®Œå…¨ç›¸åŒçš„è§£æé€»è¾‘"""
    tweets = []
    
    print(f"ğŸ” åŸå§‹å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
    print(f"ğŸ“ åŸå§‹å†…å®¹:")
    print("=" * 60)
    print(repr(content))
    print("=" * 60)
    
    # æ–¹æ³•1ï¼šJSONæ ¼å¼
    json_match = re.search(r'```json\n(.*?)\n```', content, re.DOTALL)
    if json_match:
        try:
            data = json.loads(json_match.group(1))
            if isinstance(data, list):
                tweets.extend(data)
            else:
                tweets.append(data)
            print("âœ… æ‰¾åˆ°JSONæ ¼å¼å†…å®¹")
            return tweets
        except:
            print("âŒ JSONæ ¼å¼è§£æå¤±è´¥")
            pass
    
    # æ–¹æ³•2ï¼šç»“æ„åŒ–æ–‡æœ¬æ ¼å¼
    lines = content.split('\n')
    current_tweet = {}
    
    print(f"ğŸ“„ æ€»è¡Œæ•°: {len(lines)}")
    
    for i, line in enumerate(lines):
        line = line.strip()
        print(f"ç¬¬{i+1}è¡Œ: {repr(line)}")
        
        if line.startswith('**å†…å®¹:**') or line.startswith('å†…å®¹:'):
            content_text = line.split(':', 1)[1].strip()
            # æ¸…ç†å¯èƒ½çš„æ˜Ÿå·
            content_text = content_text.lstrip('*').strip()
            current_tweet['content'] = content_text
            print(f"  â†’ æ‰¾åˆ°å†…å®¹: {repr(content_text)}")
        elif line.startswith('**è´¦å·:**') or line.startswith('è´¦å·:'):
            account_text = line.split(':', 1)[1].strip()
            # æ¸…ç†å¯èƒ½çš„æ˜Ÿå·
            account_text = account_text.lstrip('*').strip()
            current_tweet['account'] = account_text
            print(f"  â†’ æ‰¾åˆ°è´¦å·: {repr(account_text)}")
        elif line.startswith('---') and current_tweet:
            tweets.append(current_tweet)
            print(f"  â†’ æ·»åŠ æ¨æ–‡: {current_tweet}")
            current_tweet = {}
    
    # æ·»åŠ æœ€åä¸€æ¡æ¨æ–‡
    if current_tweet.get('content'):
        tweets.append(current_tweet)
        print(f"â†’ æ·»åŠ æœ€åæ¨æ–‡: {current_tweet}")
    
    # æ–¹æ³•3ï¼šç®€å•æ ¼å¼ï¼ˆå¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æ„åŒ–å†…å®¹ï¼‰
    if not tweets:
        print("\nğŸ”„ å°è¯•ç®€å•æ ¼å¼è§£æ...")
        
        # æŸ¥æ‰¾å¯èƒ½çš„æ¨æ–‡å†…å®¹
        content_lines = [line.strip() for line in lines if line.strip() and not line.startswith('#')]
        
        print(f"ğŸ“‹ è¿‡æ»¤åçš„å†…å®¹è¡Œæ•°: {len(content_lines)}")
        for i, line in enumerate(content_lines):
            print(f"  å†…å®¹è¡Œ{i+1}: {repr(line)}")
        
        if content_lines:
            # å–æ‰€æœ‰å†…å®¹ï¼Œä½†æ£€æŸ¥é•¿åº¦é™åˆ¶
            full_content = '\n'.join(content_lines)
            print(f"\nğŸ“ åˆå¹¶åçš„å®Œæ•´å†…å®¹é•¿åº¦: {len(full_content)}")
            print(f"ğŸ“ åˆå¹¶åçš„å®Œæ•´å†…å®¹:")
            print("=" * 60)
            print(repr(full_content))
            print("=" * 60)
            
            # å¦‚æœå†…å®¹è¶…è¿‡280å­—ç¬¦ï¼Œæˆªæ–­å¹¶æ·»åŠ æç¤º
            if len(full_content) > 280:
                truncated_content = full_content[:270] + "...[å†…å®¹è¿‡é•¿å·²æˆªæ–­]"
                print(f"âš ï¸ å†…å®¹è¶…é•¿({len(full_content)}å­—ç¬¦)ï¼Œå·²æˆªæ–­ä¸º: {truncated_content}")
                tweets.append({
                    'content': truncated_content,
                    'account': 'ContextSpace'  # é»˜è®¤è´¦å·
                })
            else:
                tweets.append({
                    'content': full_content,
                    'account': 'ContextSpace'  # é»˜è®¤è´¦å·
                })
    
    print("\nğŸ¯ æœ€ç»ˆè§£æç»“æœ:")
    print(f"  æ¨æ–‡æ•°é‡: {len(tweets)}")
    for i, tweet in enumerate(tweets):
        content_text = tweet.get('content', '')
        print(f"  æ¨æ–‡{i+1}:")
        print(f"    è´¦å·: {tweet.get('account', 'N/A')}")
        print(f"    å†…å®¹é•¿åº¦: {len(content_text)} å­—ç¬¦")
        print(f"    å†…å®¹: {repr(content_text)}")
    
    return tweets

def test_github_issue_markdown_format():
    """æµ‹è¯•GitHub Issueä¸­åŒ…å«Markdownæ ‡é¢˜çš„æ ¼å¼"""
    print("ğŸ§ª æµ‹è¯•: GitHub IssueåŒ…å«Markdownæ ‡é¢˜çš„æƒ…å†µ")
    print("=" * 80)
    
    # æ¨¡æ‹Ÿç”¨æˆ·åœ¨GitHub Issueæ¨¡æ¿ä¸­ç›´æ¥å†™å†…å®¹çš„æƒ…å†µ
    test_content = """## ğŸ“ æ¨æ–‡å†…å®¹

å¸¸è§çš„æä¾›å…è´¹ä»£ç†çš„ç½‘ç«™ï¼š
https://www.freeproxylists.net
https://www.hidemyass.com
https://www.proxyscrape.com
http://spys.one
https://www.sslproxies.org
https://www.kproxy.com
https://whoer.net

## ğŸ¤– è‡ªåŠ¨å‘å¸ƒè¯´æ˜

æ­¤Issueä¼šè‡ªåŠ¨è§¦å‘æ¨æ–‡å‘å¸ƒæµç¨‹ï¼š
- ç³»ç»Ÿä¼šè§£æä¸Šè¿°å†…å®¹å¹¶å‘å¸ƒåˆ°æŒ‡å®šçš„Twitterè´¦å·

å‘å¸ƒæ—¶é—´: 2024-01-17 10:30:00"""
    
    result = parse_issue_content_exact(test_content)
    return result

def test_clean_format():
    """æµ‹è¯•çº¯å‡€çš„å¤šè¡Œå†…å®¹"""
    print("\nğŸ§ª æµ‹è¯•: çº¯å‡€çš„å¤šè¡Œå†…å®¹")
    print("=" * 80)
    
    test_content = """å¸¸è§çš„æä¾›å…è´¹ä»£ç†çš„ç½‘ç«™ï¼š
https://www.freeproxylists.net
https://www.hidemyass.com
https://www.proxyscrape.com
http://spys.one
https://www.sslproxies.org
https://www.kproxy.com
https://whoer.net"""
    
    result = parse_issue_content_exact(test_content)
    return result

def test_structured_format():
    """æµ‹è¯•ç»“æ„åŒ–æ ¼å¼"""
    print("\nğŸ§ª æµ‹è¯•: ç»“æ„åŒ–æ ¼å¼")
    print("=" * 80)
    
    test_content = """**å†…å®¹:** å¸¸è§çš„æä¾›å…è´¹ä»£ç†çš„ç½‘ç«™ï¼š
https://www.freeproxylists.net
https://www.hidemyass.com
https://www.proxyscrape.com
http://spys.one
https://www.sslproxies.org
https://www.kproxy.com
https://whoer.net
**è´¦å·:** ContextSpace"""
    
    result = parse_issue_content_exact(test_content)
    return result

def test_mixed_content():
    """æµ‹è¯•åŒ…å«å…¶ä»–å†…å®¹çš„æ··åˆæ ¼å¼"""
    print("\nğŸ§ª æµ‹è¯•: åŒ…å«å…¶ä»–å†…å®¹çš„æ··åˆæ ¼å¼")
    print("=" * 80)
    
    test_content = """# æ¨æ–‡å‘å¸ƒè¯·æ±‚

æˆ‘è¦å‘å¸ƒä»¥ä¸‹å†…å®¹ï¼š

å¸¸è§çš„æä¾›å…è´¹ä»£ç†çš„ç½‘ç«™ï¼š
https://www.freeproxylists.net
https://www.hidemyass.com
https://www.proxyscrape.com
http://spys.one
https://www.sslproxies.org
https://www.kproxy.com
https://whoer.net

è¯·å‘å¸ƒåˆ°ContextSpaceè´¦å·ã€‚

è°¢è°¢ï¼"""
    
    result = parse_issue_content_exact(test_content)
    return result

if __name__ == "__main__":
    print("ğŸ” GitHub IssueçœŸå®æ ¼å¼å†…å®¹è§£ææµ‹è¯•")
    print("=" * 80)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results = []
    results.append(test_github_issue_markdown_format())
    results.append(test_clean_format())
    results.append(test_structured_format())
    results.append(test_mixed_content())
    
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print("=" * 80)
    for i, result in enumerate(results, 1):
        if result and len(result) > 0:
            content = result[0].get('content', '')
            length = len(content)
            print(f"æµ‹è¯•{i}: è§£ææˆåŠŸ - {length}å­—ç¬¦")
            if length < 100:  # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯é—®é¢˜
                print(f"  âš ï¸  å†…å®¹å¯èƒ½ä¸å®Œæ•´: {repr(content[:50])}")
            else:
                print(f"  âœ… å†…å®¹å®Œæ•´: {repr(content[:50])}...")
        else:
            print(f"æµ‹è¯•{i}: âŒ è§£æå¤±è´¥")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ") 